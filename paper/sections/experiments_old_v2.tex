\subsection{Dataset}

We evaluate our approach on CIFAR-10~\cite{krizhevsky2009learning}, a widely-used benchmark for image classification. CIFAR-10 consists of 60,000 32Ã—32 color images in 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck), with 50,000 training images and 10,000 test images. We split the training set into 45,000 for training and 5,000 for validation.

\subsection{Model Architectures}

We train five diverse CNN architectures to create heterogeneous ensembles:

\begin{itemize}
\item \textbf{ResNet-18, ResNet-34}~\cite{he2016deep}: Residual networks with different depths
\item \textbf{VGG-16}~\cite{simonyan2014very}: Classic deep architecture with small filters
\item \textbf{MobileNet-V2}~\cite{sandler2018mobilenetv2}: Efficient architecture for mobile devices
\item \textbf{DenseNet-121}~\cite{huang2017densely}: Dense connections between layers
\end{itemize}

We use pre-trained weights from ImageNet and fine-tune on CIFAR-10 for 10 epochs with learning rate 0.001, batch size 64, and Adam optimizer. This transfer learning approach reduces training time while maintaining good performance.

\subsection{Baseline Methods}

We compare our DS-based fusion against:

\begin{itemize}
\item \textbf{Individual Models}: Each model's standalone performance
\item \textbf{Simple Averaging}: Average softmax probabilities across models
\item \textbf{Voting}: Majority vote of model predictions
\item \textbf{Weighted Averaging}: Weight models by validation accuracy
\end{itemize}

\subsection{Evaluation Metrics}

\textbf{Classification Accuracy}: Percentage of correct predictions on test set.

\textbf{Expected Calibration Error (ECE)}~\cite{guo2017calibration}: Measures calibration quality:
\begin{equation}
ECE = \sum_{m=1}^M \frac{|B_m|}{n} |\text{acc}(B_m) - \text{conf}(B_m)|
\end{equation}
where $B_m$ are confidence bins, $n$ is the number of samples, acc is accuracy, and conf is average confidence.

\textbf{Uncertainty Quality Metrics}:
\begin{itemize}
\item Average belief, plausibility, and interval width
\item Correlation between uncertainty and prediction errors
\item Conflict measure distribution and its correlation with correctness
\end{itemize}

\subsection{Implementation Details}

We implement our framework in PyTorch. All experiments use:
\begin{itemize}
\item Random seed: 42 (for reproducibility)
\item Data augmentation: Random crop, horizontal flip
\item Normalization: Channel-wise mean and std from CIFAR-10
\item Hardware: CPU (models are lightweight enough)
\end{itemize}

For DS fusion, we test:
\begin{itemize}
\item \textbf{Direct assignment} with no temperature scaling
\item \textbf{Temperature-scaled} with $T = 1.5$ (smoother distributions)
\item \textbf{Calibrated assignment} using square-root normalization
\end{itemize}

\subsection{Ablation Studies}

We conduct ablation studies to analyze:

\begin{enumerate}
\item \textbf{Effect of ensemble size}: Performance with 2, 3, 4, and 5 models
\item \textbf{Belief assignment strategy}: Comparing direct, temperature-scaled, and calibrated
\item \textbf{Temperature parameter}: Testing $T \in \{0.5, 1.0, 1.5, 2.0\}$
\item \textbf{Model diversity}: Impact of using similar vs. diverse architectures
\end{enumerate}

All experiments are repeated with 3 random seeds to ensure statistical significance. We report mean and standard deviation where applicable.
