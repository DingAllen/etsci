\subsection{Summary of Key Findings}

Our comprehensive experimental evaluation demonstrates three primary findings that validate the effectiveness of DS-based ensemble fusion:

\textbf{Finding 1: Improved Accuracy through Principled Fusion.} DS fusion achieves 92.3\% accuracy on CIFAR-10, outperforming simple averaging (91.5\%), voting (91.2\%), and all individual models (best: 90.8\%). This 0.8-1.1 percentage point improvement over traditional ensembles demonstrates that principled evidence combination yields measurable performance gains. The improvement stems from DS theory's ability to weight evidence based on conflict and resolve contradictions systematically.

\textbf{Finding 2: Meaningful Uncertainty Quantification.} Our conflict measure exhibits strong correlation with prediction errors, with incorrect predictions showing 0.36 higher conflict than correct ones ($p < 0.001$). This statistically significant relationship validates DS theory's capability to identify uncertain predictions. The belief-plausibility intervals provide actionable confidence bounds, enabling threshold-based decision making for safety-critical applications.

\textbf{Finding 3: Practical Computational Efficiency.} Despite DS fusion's theoretical complexity, computational overhead remains minimal (0.12 ms per sample, representing < 1\% of end-to-end latency). This efficiency makes the approach deployable in real-world systems where both accuracy and uncertainty quantification matter.

\subsection{Theoretical and Practical Advantages}

Compared to traditional ensemble methods, our DS-based approach offers several distinct advantages:

\textbf{Explicit Uncertainty Representation:} Unlike probability averaging which produces point estimates, DS fusion generates belief-plausibility intervals that explicitly bound prediction confidence. This interval representation naturally captures epistemic uncertainty arising from model disagreement.

\textbf{Conflict Detection and Resolution:} The conflict measure $\kappa$ provides direct insight into model disagreement. High conflict signals ambiguous samples requiring careful handling, while low conflict indicates consensus. This information guides decision-making policies in applications where selective processing or human review is necessary.

\textbf{Mathematical Rigor:} DS theory provides axiomatic foundations for evidence combination, unlike heuristic fusion methods. Dempster's rule satisfies desirable properties including commutativity, associativity, and preservation of independence, ensuring consistent and interpretable fusion behavior.

\textbf{Adaptive Reliability Weighting:} Through discount factors, DS fusion naturally incorporates model-specific reliability. Less accurate models contribute reduced mass to specific hypotheses and increased mass to ignorance, preventing unreliable predictions from dominating the ensemble.

\textbf{Calibration Improvement:} As demonstrated in Figure~\ref{fig:calibration}, DS fusion achieves superior calibration compared to simple averaging. The explicit uncertainty modeling and conflict-based adjustment prevent overconfidence, a critical advantage for trustworthy AI systems.

\subsection{Implications for Safety-Critical Applications}

The strong conflict-error correlation (0.36 difference) has important implications for deploying ensemble systems in high-stakes domains:

\textbf{Medical Diagnosis:} High-conflict predictions could trigger additional testing or specialist review, reducing misdiagnosis risk while maintaining efficiency for clear cases.

\textbf{Autonomous Driving:} Conflict-based uncertainty could modulate vehicle behavior, increasing caution when perception systems disagree on scene interpretation.

\textbf{Security Systems:} Uncertain predictions in threat detection could invoke human verification, balancing security and usability.

\textbf{Financial Risk Assessment:} Prediction intervals could inform risk-adjusted decision making, with wider intervals signaling need for additional analysis.

In each domain, DS fusion's interpretable uncertainty metrics enable nuanced decision policies impossible with confidence-less ensembles.

\subsection{Insights from Ablation Studies}

Our ablation studies (Figure~\ref{fig:ablation}) reveal important design principles:

\textbf{Ensemble Size Optimization:} The diminishing returns beyond 4-5 models suggest an optimal trade-off point. For resource-constrained deployments, a carefully selected 3-4 model ensemble may provide 95\% of the benefit with 40-50\% of the computational cost.

\textbf{Temperature Selection:} The peak performance at $T=1.0$ indicates that for well-calibrated neural networks (common with modern architectures and training procedures), direct belief assignment suffices. Temperature scaling becomes valuable primarily for overconfident or poorly calibrated base models.

\textbf{Importance of Diversity:} The 4.4 percentage point gap between diverse and homogeneous ensembles (Figure~\ref{fig:ablation}d) underscores that architectural diversity is as important as ensemble size. Combining complementary architectures (e.g., ResNet's skip connections, VGG's depth, MobileNet's efficiency) yields richer evidence than simply duplicating similar models.

\textbf{Assignment Strategy Robustness:} The similar performance of different assignment strategies (direct: 92.3\%, calibrated: 91.9\%, temperature: 91.8\%) indicates robustness to this design choice. Practitioners can select the simplest option (direct) without sacrificing performance.

\subsection{Comparison with Recent Work}

Recent work~\cite{arxiv2024feature} explored DS theory for CNN ensemble fusion on CIFAR-10/100, focusing on feature-level fusion. Our approach differs in three key aspects:

\textbf{Model-Level vs. Feature-Level:} We operate on model outputs rather than internal features, making our approach:
\begin{itemize}
\item Compatible with pre-trained models without architecture modification
\item Applicable to black-box models where internal features are inaccessible
\item Computationally lighter (no feature extraction overhead)
\end{itemize}

\textbf{Comprehensive Uncertainty Analysis:} We provide extensive analysis of conflict-error correlation, calibration quality, and uncertainty intervals—dimensions not explored in prior work.

\textbf{Practical Deployment Considerations:} Our computational cost analysis and ablation studies provide actionable guidance for practitioners, addressing the gap between theoretical methods and real-world deployment.

Compared to evidential deep learning~\cite{sensoy2018evidential}, which parameterizes Dirichlet distributions, our approach:
\begin{itemize}
\item Uses classical DS combination rules, providing clearer interpretability
\item Requires no model retraining (works with standard softmax outputs)
\item Offers explicit conflict detection unavailable in evidential networks
\end{itemize}

\subsection{Terminology Clarification: ``Adaptive'' Fusion}

We acknowledge that the term ``adaptive'' in our title warrants clarification, as noted by the reviewer.

\textbf{Current Approach:} Our reliability weighting (discount factors $r_i$) is computed once on the validation set and remains \textit{static} during inference. This represents ``validation-based adaptive weighting'' rather than ``sample-adaptive'' or ``dynamic'' weighting that adjusts per input.

\textbf{Justification for Terminology:} We use ``adaptive'' to distinguish from uniform weighting (where all models contribute equally). Our validation-based approach \textit{adapts} model contributions based on historical performance, albeit in a pre-computed manner.

\textbf{More Precise Alternatives:} For clarity, this approach could alternatively be termed:
\begin{itemize}
\item ``Reliability-Weighted DS Fusion''
\item ``Validation-Calibrated DS Ensemble''
\item ``Performance-Adjusted DS Combination''
\end{itemize}

\textbf{Future Work—Truly Dynamic Adaptation:} A natural extension involves \textit{instance-specific} adaptation where discount factors vary per sample based on:
\begin{itemize}
\item Input complexity metrics (edge density, texture variance)
\item Model-specific confidence on the current input
\item Local reliability estimated from similar training samples
\end{itemize}

Such dynamic weighting could further improve performance but requires additional computational overhead and careful validation. Our current static approach provides a practical balance of performance and simplicity.

\subsection{Model Correlation and Its Effects on DS Fusion}

An important theoretical consideration, raised by the reviewer, is that DS theory assumes \textit{independent evidence sources}. However, our CNN models are trained on the same dataset, potentially introducing correlation in their predictions—especially their errors.

\textbf{Evidence of Correlation:} Examining error overlap, we find:
\begin{itemize}
\item \textbf{Agreement on Errors}: 34\% of errors are shared by $\geq$3 models
\item \textbf{Correlated Uncertainty}: Challenging classes (cat/dog) induce similar confusion across models
\item \textbf{Dataset Bias}: Systematic biases in CIFAR-10 (e.g., green backgrounds for frogs) affect all models similarly
\end{itemize}

\textbf{Impact on Conflict Measure:} High correlation can suppress conflict $\kappa$, potentially causing ``overconfident consensus errors''—cases where models jointly misclassify with low detected conflict. We estimate this occurs in ~5-8\% of misclassifications.

\textbf{Mitigation Strategies We Employ:}
\begin{itemize}
\item \textbf{Architectural Diversity}: Using 5 different architectures (ResNet, VGG, MobileNet, DenseNet) reduces correlation compared to homogeneous ensembles
\item \textbf{Different Training Procedures}: Models differ in depth, optimization details, and initialization
\item \textbf{Conflict Threshold Calibration}: Our rejection policy (Section~\ref{sec:rejection}) accounts for baseline conflict levels
\end{itemize}

\textbf{Theoretical Perspective:} While full independence is rarely achieved in practice, empirical validation (Section~\ref{sec:results}) shows DS fusion still provides:
\begin{itemize}
\item Superior calibration (ECE: 0.011) compared to methods that ignore correlation
\item Strong conflict-error correlation (0.36 difference), indicating conflict remains informative
\item Practical utility in selective prediction (99.8\% accuracy at 80\% coverage)
\end{itemize}

\textbf{Future Work:} Explicitly modeling correlation in DS fusion could improve performance:
\begin{itemize}
\item Correlation-adjusted conflict normalization
\item Covariance-aware evidence combination (extending Dempster's rule)
\item Diversity-promoting ensemble construction guided by correlation analysis
\end{itemize}

This discussion acknowledges the theoretical gap while demonstrating that practical performance remains strong despite imperfect independence.

\subsection{Limitations and Future Directions}

While promising, our approach has limitations that suggest future research directions:

\textbf{Computational Scalability:} For very large ensembles (>10 models) or high-dimensional output spaces (>1000 classes), the number of focal sets in Dempster's combination can grow large. Future work could explore:
\begin{itemize}
\item Approximation techniques for large-scale fusion
\item Hierarchical combination strategies to reduce complexity
\item GPU-accelerated implementation for parallel conflict computation
\end{itemize}

\textbf{Theoretical Guarantees:} While DS theory provides axiomatic foundations, establishing PAC-style generalization bounds for DS ensemble fusion remains an open problem. Theoretical analysis connecting conflict measures to generalization error could strengthen the approach's foundations.

\textbf{Dynamic Weighting:} Our current discount factors are fixed based on validation accuracy. Instance-specific, confidence-aware weighting could improve fusion quality:
\begin{itemize}
\item Local model reliability estimation based on input characteristics
\item Meta-learning approaches to predict optimal discount factors
\item Adaptive weighting based on training dynamics and diversity
\end{itemize}

\textbf{Extension to Other Tasks:} While demonstrated on classification, the framework generalizes to:
\begin{itemize}
\item Object detection with bounding box uncertainty
\item Semantic segmentation with pixel-wise confidence
\item Multi-modal fusion (vision + language, vision + lidar)
\item Structured prediction with compositional uncertainty
\end{itemize}

\textbf{Calibration Analysis:} Deeper investigation of the relationship between DS fusion and calibration could yield:
\begin{itemize}
\item Theoretical analysis of calibration properties
\item Adaptive temperature selection based on calibration metrics
\item Comparison with explicit calibration methods (Platt scaling, isotonic regression)
\end{itemize}

\subsection{Practical Recommendations}

Based on our findings, we offer practitioners the following guidance for deploying DS-based ensemble fusion:

\begin{enumerate}
\item \textbf{Start with Direct Assignment:} Use the simple probability-to-mass mapping unless base models are poorly calibrated.

\item \textbf{Prioritize Diversity:} Invest in diverse architectures (3-5 models) rather than many similar ones.

\item \textbf{Monitor Conflict:} Track conflict distributions in production; shifts may indicate distribution drift or adversarial inputs.

\item \textbf{Set Confidence Thresholds:} Use conflict > 0.7 or interval width > 0.2 as flags for uncertain predictions requiring review.

\item \textbf{Balance Cost and Accuracy:} For resource-constrained settings, 3-4 carefully selected models provide most benefits.

\item \textbf{Validate Calibration:} Periodically check calibration quality; recalibrate base models if necessary.
\end{enumerate}

These guidelines balance theoretical principles with practical deployment considerations, enabling effective use of DS fusion in real-world systems.