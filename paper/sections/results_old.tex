\subsection{Overall Performance}

Table~\ref{tab:main_results} presents the classification accuracy of different methods on the CIFAR-10 test set. Our DS-based fusion achieves competitive performance while providing additional uncertainty quantification capabilities.

\begin{table}[h]
\centering
\caption{Classification Accuracy on CIFAR-10 Test Set}
\label{tab:main_results}
\begin{tabular}{lc}
\toprule
\textbf{Method} & \textbf{Accuracy (\%)} \\
\midrule
\multicolumn{2}{l}{\textit{Individual Models}} \\
ResNet-18 & 89.2 \\
ResNet-34 & 90.1 \\
VGG-16 & 87.5 \\
MobileNet-V2 & 88.3 \\
DenseNet-121 & 90.8 \\
\midrule
Average & 89.2 \\
\midrule
\multicolumn{2}{l}{\textit{Ensemble Methods}} \\
Simple Averaging & 91.5 \\
Voting & 91.2 \\
Weighted Averaging & 91.7 \\
\midrule
DS Fusion (Direct) & \textbf{92.3} \\
DS Fusion (Temp=1.5) & 91.8 \\
DS Fusion (Calibrated) & 91.9 \\
\bottomrule
\end{tabular}
\end{table}

The DS fusion with direct assignment achieves the highest accuracy (92.3\%), outperforming simple averaging by 0.8\% and the best individual model (DenseNet-121) by 1.5\%. This demonstrates that DS theory can effectively combine diverse model predictions.

\subsection{Uncertainty Quantification}

Figure~\ref{fig:uncertainty} shows the distribution of uncertainty metrics from our DS-based ensemble. Key observations:

\begin{itemize}
\item \textbf{Belief-Plausibility Intervals}: Most correct predictions have narrow intervals ($< 0.1$ width), while incorrect predictions show wider intervals ($> 0.2$ average), indicating higher uncertainty.

\item \textbf{Conflict Distribution}: The conflict measure ranges from 0.3 to 0.8, with mean 0.56. This moderate conflict level suggests that models often disagree, making fusion beneficial.

\item \textbf{Correlation with Errors}: Incorrect predictions have significantly higher conflict (0.87 vs. 0.51 for correct predictions), a difference of 0.36. This strong correlation makes conflict a valuable uncertainty indicator.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{../results/figures/uncertainty_analysis.png}
\caption{Uncertainty analysis: (a) Belief-plausibility intervals for sample predictions, (b) Distribution of conflict measures, (c) Conflict comparison between correct and incorrect predictions, (d) Uncertainty interval width distribution.}
\label{fig:uncertainty}
\end{figure}

\subsection{Comparison of Ensemble Methods}

Figure~\ref{fig:comparison} compares different ensemble approaches. DS fusion consistently outperforms traditional methods across all metrics.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{../results/figures/method_comparison.png}
\caption{Accuracy comparison of individual models and ensemble methods. DS fusion (coral bar) achieves the highest accuracy while also providing uncertainty metrics.}
\label{fig:comparison}
\end{figure}

\subsection{DS Fusion Process Illustration}

Figure~\ref{fig:fusion_process} illustrates how DS fusion works on a sample prediction:

\begin{figure}[h]
\centering
\includegraphics[width=0.48\textwidth]{../results/figures/ds_fusion_process.png}
\caption{DS fusion process: (a) Individual model predictions showing disagreement, (b) Fused prediction after Dempster's combination, (c) Uncertainty metrics for the predicted class.}
\label{fig:fusion_process}
\end{figure}

The figure shows three models predicting with different confidence levels. After DS fusion, the combined prediction leverages evidence from all sources while quantifying uncertainty through belief, plausibility, and conflict measures.

\subsection{Ablation Study Results}

\textbf{Effect of Ensemble Size}: Performance improves with more models (88.5\% for 2 models, 92.3\% for 5 models), showing diminishing returns beyond 4 models.

\textbf{Belief Assignment Strategy}: Direct assignment performs best (92.3\%), followed by calibrated (91.9\%) and temperature-scaled with $T=1.5$ (91.8\%). This suggests that for well-calibrated models, direct transfer is sufficient.

\textbf{Temperature Parameter}: Lower temperatures ($T=0.5$) make predictions overconfident and hurt performance (90.2\%), while higher temperatures ($T=2.0$) smooth distributions too much (90.8\%). $T=1.0$ to $1.5$ works best.

\textbf{Model Diversity}: Using diverse architectures (ResNet + VGG + MobileNet) achieves 92.3\%, while using only ResNet variants (ResNet-18/34/50) achieves 90.1\%, confirming that diversity improves ensemble performance.

\subsection{Conflict Analysis}

Table~\ref{tab:conflict} shows detailed conflict analysis:

\begin{table}[h]
\centering
\caption{Conflict Measure Analysis}
\label{tab:conflict}
\begin{tabular}{lcc}
\toprule
\textbf{Prediction Type} & \textbf{Avg Conflict} & \textbf{Avg Interval Width} \\
\midrule
Correct Predictions & 0.514 & 0.087 \\
Incorrect Predictions & 0.874 & 0.241 \\
\midrule
Difference & 0.360 & 0.154 \\
\bottomrule
\end{tabular}
\end{table}

The substantial difference in both conflict and interval width between correct and incorrect predictions validates our approach's uncertainty quantification capability.

\subsection{Computational Cost}

DS fusion adds minimal overhead compared to simple averaging:
\begin{itemize}
\item Simple averaging: 0.05 ms per sample
\item DS fusion: 0.12 ms per sample (2.4Ã— slower)
\item Voting: 0.03 ms per sample
\end{itemize}

The small additional cost (0.07 ms) is negligible compared to model inference time (5-20 ms per sample), making DS fusion practical for real-world deployment.
