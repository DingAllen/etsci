\subsection{Dataset and Training}

We evaluate on CIFAR-10~\cite{krizhevsky2009learning}, a widely-used benchmark containing 60,000 32×32 color images across 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). We split the 50,000 training images into 45,000 for training and 5,000 for validation, reserving 10,000 for testing.

\textbf{Model Architectures:} We train five diverse CNN architectures for heterogeneous ensembles: ResNet-18 and ResNet-34~\cite{he2016deep} (residual networks with varying depth), VGG-16~\cite{simonyan2014very} (classic deep architecture), MobileNetV2~\cite{sandler2018mobilenetv2} (efficient design), and DenseNet-121~\cite{huang2017densely} (dense connections). We use ImageNet pre-trained weights and fine-tune on CIFAR-10 for 10 epochs (learning rate 0.001, batch size 64, Adam optimizer).

\subsection{Baseline Comparisons}

We compare against multiple baselines:

\textbf{Individual Models:} Each CNN's standalone performance establishes lower bounds.

\textbf{Traditional Ensembles:}
\begin{itemize}
\item \textbf{Simple Averaging}: Average softmax probabilities
\item \textbf{Voting}: Majority vote across models
\item \textbf{Weighted Averaging}: Weight models by validation accuracy
\end{itemize}

\textbf{Uncertainty-Aware Methods:}
\begin{itemize}
\item \textbf{MC Dropout}~\cite{gal2016dropout}: Approximate Bayesian inference via dropout sampling (20 forward passes)
\item \textbf{Deep Ensembles}~\cite{lakshminarayanan2017simple}: Ensemble of independently trained networks
\end{itemize}

This comprehensive comparison validates DS fusion against both traditional and modern uncertainty quantification methods.

\subsection{Evaluation Metrics}

\textbf{In-Distribution Performance:}
\begin{itemize}
\item \textbf{Classification Accuracy}: Percentage of correct predictions
\item \textbf{Expected Calibration Error (ECE)}~\cite{guo2017calibration}: Measures probability calibration
\item \textbf{Uncertainty-Error Correlation}: Correlation between uncertainty scores and prediction correctness
\end{itemize}

\textbf{Out-of-Distribution (OOD) Detection:} Following best practices for uncertainty evaluation~\cite{hendrycks2016baseline}, we test OOD detection capability:
\begin{itemize}
\item \textbf{OOD Dataset}: SVHN (Street View House Numbers) as distribution shift
\item \textbf{AUROC}: Area under ROC curve for separating in-dist from OOD
\item \textbf{FPR@95}: False positive rate at 95\% true positive rate
\item \textbf{Uncertainty Distribution}: Compare in-dist vs OOD uncertainty
\end{itemize}

\textbf{Adversarial Robustness:} We evaluate uncertainty under adversarial attacks:
\begin{itemize}
\item \textbf{FGSM Attack}~\cite{goodfellow2014explaining}: Fast Gradient Sign Method with $\epsilon = 0.03$
\item \textbf{Uncertainty Increase}: Measure conflict and interval width on adversarial examples
\item \textbf{Accuracy Degradation}: Compare clean vs adversarial accuracy
\end{itemize}

\subsection{Implementation Details}

All experiments use PyTorch with random seed 42 for reproducibility. Data augmentation includes random crop and horizontal flip. We normalize using CIFAR-10 statistics. For DS fusion, we test three belief assignment strategies: direct (no scaling), temperature-scaled ($T \in \{0.5, 1.0, 1.5, 2.0\}$), and calibrated (square-root normalization).

\subsection{Ablation Studies}

We systematically analyze:
\begin{enumerate}
\item \textbf{Ensemble Size}: Performance with 1-6 models
\item \textbf{Assignment Strategy}: Direct, temperature-scaled, calibrated
\item \textbf{Temperature Parameter}: Optimal $T$ for different model types
\item \textbf{Model Diversity}: Homogeneous (same architecture) vs heterogeneous
\item \textbf{Conflict Thresholds}: Effect of adaptive handling policies
\end{enumerate}

All experiments report mean ± standard deviation across 3 random seeds.
